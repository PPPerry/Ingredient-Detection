{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/food41\"))\nprint(os.listdir(\"../input/ingredients101\"))\nfrom tqdm import tqdm, tqdm_notebook\n\n# Any results you write to the current directory are saved as output.","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:37.645242Z","iopub.execute_input":"2022-03-11T06:39:37.645571Z","iopub.status.idle":"2022-03-11T06:39:37.682597Z","shell.execute_reply.started":"2022-03-11T06:39:37.645485Z","shell.execute_reply":"2022-03-11T06:39:37.681788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input/ingredients101/Ingredients101/\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:37.684152Z","iopub.execute_input":"2022-03-11T06:39:37.684472Z","iopub.status.idle":"2022-03-11T06:39:37.69145Z","shell.execute_reply.started":"2022-03-11T06:39:37.684435Z","shell.execute_reply":"2022-03-11T06:39:37.690663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport os\nimport sys\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom glob import glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\n\nuse_cuda = True\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\ntorch.manual_seed(42) # try and make the results more reproducible\nBASE_PATH = '../input/ingredients101/Ingredients101/'","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:37.692704Z","iopub.execute_input":"2022-03-11T06:39:37.692947Z","iopub.status.idle":"2022-03-11T06:39:39.490689Z","shell.execute_reply.started":"2022-03-11T06:39:37.692914Z","shell.execute_reply":"2022-03-11T06:39:39.489977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_img_df = pd.DataFrame({'path': glob(os.path.join(BASE_PATH, 'images', '*', '*.jpg'))})\n#all_img_df['category'] = all_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\n#https://www.kaggle.com/kmader/vgg-on-food-subset-torch/data\nprint(os.listdir(\"../input/food41/images\"))\nepochs = 35\nbatch_size = 64\nMICRO_DATA = True # very small subset (just 3 groups)\nSAMPLE_TRAINING = False # make train set smaller for faster iteration\nIMG_SIZE = (384, 384)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.492135Z","iopub.execute_input":"2022-03-11T06:39:39.492392Z","iopub.status.idle":"2022-03-11T06:39:39.508312Z","shell.execute_reply.started":"2022-03-11T06:39:39.492358Z","shell.execute_reply":"2022-03-11T06:39:39.50764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ingredients for each class\nf = open(BASE_PATH + '/Annotations/ingredients_simplified.txt', \"r\")\ningredients = f.read().split('\\n')\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.510985Z","iopub.execute_input":"2022-03-11T06:39:39.511169Z","iopub.status.idle":"2022-03-11T06:39:39.519446Z","shell.execute_reply.started":"2022-03-11T06:39:39.511146Z","shell.execute_reply":"2022-03-11T06:39:39.518675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Classes\nf = open(BASE_PATH + '/Annotations/classes.txt', \"r\")\nclasses = f.read().split('\\n')\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.520854Z","iopub.execute_input":"2022-03-11T06:39:39.521315Z","iopub.status.idle":"2022-03-11T06:39:39.528165Z","shell.execute_reply.started":"2022-03-11T06:39:39.521258Z","shell.execute_reply":"2022-03-11T06:39:39.527325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Base Ingredients\nf = open(BASE_PATH + '/ingredients_simplification/baseIngredients.txt', \"r\")\nbase_ing = f.read().split('\\n')\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.52958Z","iopub.execute_input":"2022-03-11T06:39:39.529887Z","iopub.status.idle":"2022-03-11T06:39:39.539038Z","shell.execute_reply.started":"2022-03-11T06:39:39.529849Z","shell.execute_reply":"2022-03-11T06:39:39.538211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Construct dataframe for dictionary\nbase_ing = base_ing[0].split(\",\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.54077Z","iopub.execute_input":"2022-03-11T06:39:39.541042Z","iopub.status.idle":"2022-03-11T06:39:39.544771Z","shell.execute_reply.started":"2022-03-11T06:39:39.541007Z","shell.execute_reply":"2022-03-11T06:39:39.543904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_ing\ningredients\nclasses","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.546414Z","iopub.execute_input":"2022-03-11T06:39:39.546667Z","iopub.status.idle":"2022-03-11T06:39:39.559325Z","shell.execute_reply.started":"2022-03-11T06:39:39.546633Z","shell.execute_reply":"2022-03-11T06:39:39.558459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_ingredients = []\nfor arr in ingredients:\n    arr = arr.split(\",\")\n    new_ingredients.append(arr)\nnew_ingredients","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.561362Z","iopub.execute_input":"2022-03-11T06:39:39.561898Z","iopub.status.idle":"2022-03-11T06:39:39.588673Z","shell.execute_reply.started":"2022-03-11T06:39:39.561858Z","shell.execute_reply":"2022-03-11T06:39:39.588046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\n\ndf = pd.DataFrame(mlb.fit_transform(new_ingredients),columns=mlb.classes_) #binary encode ingredients","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:39.589741Z","iopub.execute_input":"2022-03-11T06:39:39.58998Z","iopub.status.idle":"2022-03-11T06:39:40.425353Z","shell.execute_reply.started":"2022-03-11T06:39:39.589939Z","shell.execute_reply":"2022-03-11T06:39:40.424644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"target\"] = classes\nfood_dict = df","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.426891Z","iopub.execute_input":"2022-03-11T06:39:40.427111Z","iopub.status.idle":"2022-03-11T06:39:40.438055Z","shell.execute_reply.started":"2022-03-11T06:39:40.427078Z","shell.execute_reply":"2022-03-11T06:39:40.436826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\nf = open(BASE_PATH + '/Annotations/train_images.txt', \"r\")\ntrain_images = f.read().split('\\n')\nf.close()\nf = open(BASE_PATH + '/Annotations/train_labels.txt', \"r\")\ntrain_labels = f.read().split('\\n')\nf.close()\n\n#val\nf = open(BASE_PATH + '/Annotations/val_images.txt', \"r\")\nval_images = f.read().split('\\n')\nf.close()\nf = open(BASE_PATH + '/Annotations/val_labels.txt', \"r\")\nval_labels = f.read().split('\\n')\nf.close()\n\n#test\nf = open(BASE_PATH + '/Annotations/test_images.txt', \"r\")\ntest_images = f.read().split('\\n')\nf.close()\nf = open(BASE_PATH + '/Annotations/test_labels.txt', \"r\")\ntest_labels = f.read().split('\\n')\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.440346Z","iopub.execute_input":"2022-03-11T06:39:40.440804Z","iopub.status.idle":"2022-03-11T06:39:40.50581Z","shell.execute_reply.started":"2022-03-11T06:39:40.440767Z","shell.execute_reply":"2022-03-11T06:39:40.505129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_images\nlen(train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.509508Z","iopub.execute_input":"2022-03-11T06:39:40.509704Z","iopub.status.idle":"2022-03-11T06:39:40.516255Z","shell.execute_reply.started":"2022-03-11T06:39:40.50968Z","shell.execute_reply":"2022-03-11T06:39:40.515371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = [\"../input/food41/images/\" + s + \".jpg\" for s in train_images]\nall_img_df = pd.DataFrame({'path': train_images, 'class_id': train_labels})\nval_images = [\"../input/food41/images/\" + s + \".jpg\" for s in val_images]\nval_img_df = pd.DataFrame({'path': val_images, 'class_id': val_labels})\ntest_images = [\"../input/food41/images/\" + s + \".jpg\" for s in test_images]\ntest_img_df = pd.DataFrame({'path': test_images, 'class_id': test_labels})","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.517829Z","iopub.execute_input":"2022-03-11T06:39:40.518137Z","iopub.status.idle":"2022-03-11T06:39:40.55985Z","shell.execute_reply.started":"2022-03-11T06:39:40.518104Z","shell.execute_reply":"2022-03-11T06:39:40.55914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_df = all_img_df[:-1]\nval_img_df = val_img_df[:-1]\ntest_img_df = test_img_df[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.561206Z","iopub.execute_input":"2022-03-11T06:39:40.561574Z","iopub.status.idle":"2022-03-11T06:39:40.565984Z","shell.execute_reply.started":"2022-03-11T06:39:40.561537Z","shell.execute_reply":"2022-03-11T06:39:40.565043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_df['class_name'] = all_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\nval_img_df['class_name'] = val_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\ntest_img_df['class_name'] = test_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.567321Z","iopub.execute_input":"2022-03-11T06:39:40.567807Z","iopub.status.idle":"2022-03-11T06:39:40.867533Z","shell.execute_reply.started":"2022-03-11T06:39:40.567771Z","shell.execute_reply":"2022-03-11T06:39:40.866816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_dict = food_dict.drop('', 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.869199Z","iopub.execute_input":"2022-03-11T06:39:40.869592Z","iopub.status.idle":"2022-03-11T06:39:40.877562Z","shell.execute_reply.started":"2022-03-11T06:39:40.869557Z","shell.execute_reply":"2022-03-11T06:39:40.876795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_dict","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.878619Z","iopub.execute_input":"2022-03-11T06:39:40.879134Z","iopub.status.idle":"2022-03-11T06:39:40.903969Z","shell.execute_reply.started":"2022-03-11T06:39:40.879089Z","shell.execute_reply":"2022-03-11T06:39:40.903331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataframe for train images\nnew_data = []\nfor index, row in all_img_df.iterrows():\n    #get binary encoding ingredients from lookup\n    food = row[\"class_name\"]\n    path = row[\"path\"]\n    class_id = row[\"class_id\"]\n    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n    binary_encod[\"path\"] = path\n    binary_encod[\"class_id\"] = class_id\n    #print(binary_encod)\n    #print((list(binary_encod.columns.values)))\n    #print(len(np.array(binary_encod)[0]))\n    new_data.append(np.array(binary_encod)[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:39:40.90508Z","iopub.execute_input":"2022-03-11T06:39:40.905804Z","iopub.status.idle":"2022-03-11T06:41:12.521566Z","shell.execute_reply.started":"2022-03-11T06:39:40.905766Z","shell.execute_reply":"2022-03-11T06:41:12.520758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = list(binary_encod.columns.values)\ntrain_df = pd.DataFrame(new_data, columns = col_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:41:12.522828Z","iopub.execute_input":"2022-03-11T06:41:12.523059Z","iopub.status.idle":"2022-03-11T06:41:18.545658Z","shell.execute_reply.started":"2022-03-11T06:41:12.523026Z","shell.execute_reply":"2022-03-11T06:41:18.544912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataframe for val images\nval_data = []\nfor index, row in val_img_df.iterrows():\n    #get binary encoding ingredients from lookup\n    food = row[\"class_name\"]\n    path = row[\"path\"]\n    class_id = row[\"class_id\"]\n    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n    binary_encod[\"path\"] = path\n    binary_encod[\"class_id\"] = int(class_id)\n    val_data.append(np.array(binary_encod)[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:41:18.546761Z","iopub.execute_input":"2022-03-11T06:41:18.548761Z","iopub.status.idle":"2022-03-11T06:41:28.913241Z","shell.execute_reply.started":"2022-03-11T06:41:18.54873Z","shell.execute_reply":"2022-03-11T06:41:28.912546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = pd.DataFrame(val_data, columns = col_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:41:28.914566Z","iopub.execute_input":"2022-03-11T06:41:28.914832Z","iopub.status.idle":"2022-03-11T06:41:29.593638Z","shell.execute_reply.started":"2022-03-11T06:41:28.914794Z","shell.execute_reply":"2022-03-11T06:41:29.59289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataframe for test images\ntest_data = []\nfor index, row in test_img_df.iterrows():\n    #get binary encoding ingredients from lookup\n    food = row[\"class_name\"]\n    path = row[\"path\"]\n    class_id = row[\"class_id\"]\n    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n    binary_encod[\"path\"] = path\n    binary_encod[\"class_id\"] = int(class_id)\n    test_data.append(np.array(binary_encod)[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:41:29.595126Z","iopub.execute_input":"2022-03-11T06:41:29.595431Z","iopub.status.idle":"2022-03-11T06:42:04.979402Z","shell.execute_reply.started":"2022-03-11T06:41:29.59539Z","shell.execute_reply":"2022-03-11T06:42:04.978697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame(test_data, columns = col_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:42:04.980742Z","iopub.execute_input":"2022-03-11T06:42:04.980984Z","iopub.status.idle":"2022-03-11T06:42:07.64105Z","shell.execute_reply.started":"2022-03-11T06:42:04.980949Z","shell.execute_reply":"2022-03-11T06:42:07.640322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tables\ntrain_df.to_hdf('train_df.h5','df',mode='w',format='table',data_columns=True)\nval_df.to_hdf('val_df.h5','df',mode='w',format='table',data_columns=True)\ntest_df.to_hdf('test_df.h5','df',mode='w',format='table',data_columns=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:42:07.645328Z","iopub.execute_input":"2022-03-11T06:42:07.648442Z","iopub.status.idle":"2022-03-11T06:43:04.289471Z","shell.execute_reply.started":"2022-03-11T06:42:07.648397Z","shell.execute_reply":"2022-03-11T06:43:04.288654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook, tnrange","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:04.291575Z","iopub.execute_input":"2022-03-11T06:43:04.291782Z","iopub.status.idle":"2022-03-11T06:43:04.298966Z","shell.execute_reply.started":"2022-03-11T06:43:04.291756Z","shell.execute_reply":"2022-03-11T06:43:04.298162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2\nbatch_size = 64\nSMALL_DATA = False\nIMG_SIZE = (384, 384)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:04.300365Z","iopub.execute_input":"2022-03-11T06:43:04.300906Z","iopub.status.idle":"2022-03-11T06:43:04.305044Z","shell.execute_reply.started":"2022-03-11T06:43:04.300868Z","shell.execute_reply":"2022-03-11T06:43:04.304418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input/food41/images\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:04.306135Z","iopub.execute_input":"2022-03-11T06:43:04.306644Z","iopub.status.idle":"2022-03-11T06:43:04.317955Z","shell.execute_reply.started":"2022-03-11T06:43:04.306607Z","shell.execute_reply":"2022-03-11T06:43:04.317216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_hdf(\"./train_df.h5\")\nval_df = pd.read_hdf(\"./val_df.h5\")\ntest_df = pd.read_hdf(\"./test_df.h5\")\n\nif SMALL_DATA:\n    train_df = train_df[:128]\n    val_df = val_df[:128]\n    test_df = test_df[:128]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:04.3191Z","iopub.execute_input":"2022-03-11T06:43:04.320018Z","iopub.status.idle":"2022-03-11T06:43:07.574644Z","shell.execute_reply.started":"2022-03-11T06:43:04.319981Z","shell.execute_reply":"2022-03-11T06:43:07.57389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = list(train_df.columns.values)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:07.576248Z","iopub.execute_input":"2022-03-11T06:43:07.576505Z","iopub.status.idle":"2022-03-11T06:43:07.581293Z","shell.execute_reply.started":"2022-03-11T06:43:07.57647Z","shell.execute_reply":"2022-03-11T06:43:07.58024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ing_names = col_names[:-3]\ntargets = ing_names","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:07.582797Z","iopub.execute_input":"2022-03-11T06:43:07.583055Z","iopub.status.idle":"2022-03-11T06:43:07.589643Z","shell.execute_reply.started":"2022-03-11T06:43:07.583019Z","shell.execute_reply":"2022-03-11T06:43:07.588986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataWrapper(data.Dataset):\n    ''' Data wrapper for pytorch's data loader function '''\n    def __init__(self, image_df, resize):\n        self.dataset = image_df\n        self.resize = resize\n\n    def __getitem__(self, index):\n        c_row = self.dataset.iloc[index]\n        target_arr = []\n        for item in c_row[targets].values:\n            target_arr.append(item)\n        #print(target_arr)\n        image_path, target = c_row['path'], torch.from_numpy(np.array(target_arr)).float()  #image and target\n        #read as rgb image, resize and convert to range 0 to 1\n        image = cv2.imread(image_path, 1)\n        if self.resize:\n            image = cv2.resize(image, IMG_SIZE)/255.0 \n        else:\n            image = image/255.0\n        image = (torch.from_numpy(image.transpose(2,0,1))).float() #NxCxHxW\n        return image, target\n\n    def __len__(self):\n        return self.dataset.shape[0] ","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:07.591026Z","iopub.execute_input":"2022-03-11T06:43:07.591517Z","iopub.status.idle":"2022-03-11T06:43:07.6008Z","shell.execute_reply.started":"2022-03-11T06:43:07.591481Z","shell.execute_reply":"2022-03-11T06:43:07.600119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet50(pretrained=True)\n# #freeze layers\n# for param in model.parameters():\n#      param.requires_grad = False\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(targets))\n\nct = 0\nfor name, child in model.named_children():\n    ct += 1\n    if ct < 8:\n        for name2, params in child.named_parameters():\n            params.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:07.602179Z","iopub.execute_input":"2022-03-11T06:43:07.602743Z","iopub.status.idle":"2022-03-11T06:43:14.696973Z","shell.execute_reply.started":"2022-03-11T06:43:07.602705Z","shell.execute_reply":"2022-03-11T06:43:14.696206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nmodel = model.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\ntrain_dataset = DataWrapper(train_df, True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True, batch_size=batch_size, pin_memory=False)\n\nval_dataset = DataWrapper(val_df, True)\nval_loader = torch.utils.data.DataLoader(val_dataset,shuffle=True, batch_size=batch_size, pin_memory=False)\n\ntest_dataset = DataWrapper(test_df, True)\ntest_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True, batch_size=batch_size, pin_memory=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:14.698471Z","iopub.execute_input":"2022-03-11T06:43:14.698873Z","iopub.status.idle":"2022-03-11T06:43:17.658738Z","shell.execute_reply.started":"2022-03-11T06:43:14.698831Z","shell.execute_reply":"2022-03-11T06:43:17.657988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRY LATER\ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    https://stackoverflow.com/q/32239577/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:17.659978Z","iopub.execute_input":"2022-03-11T06:43:17.66022Z","iopub.status.idle":"2022-03-11T06:43:17.667428Z","shell.execute_reply.started":"2022-03-11T06:43:17.660188Z","shell.execute_reply":"2022-03-11T06:43:17.666668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\ntrain_results = defaultdict(list)\ntrain_iter, test_iter, best_acc = 0,0,0\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = (10, 10))\nax1.set_title('Train Loss')\nax2.set_title('Train Accuracy')\nax3.set_title('Test Loss')\nax4.set_title('Test Accuracy')\n\nf1_scores = defaultdict(list)\n\nfor i in tnrange(epochs, desc='Epochs'):\n    print(\"Epoch \",i)\n    ## Train Phase\n    #Model switches to train phase\n    model.train() \n    \n    all_outputs = []\n    all_targets = []\n    # Running through all mini batches in the dataset\n    count, loss_val, correct, total = train_iter, 0, 0, 0\n    for img_data, target in tqdm_notebook(train_loader, desc='Training'):    \n        img_data, target = img_data.to(device), target.to(device)\n        \n        output = model(img_data) #FWD prop\n\n        loss = criterion(output, target) #Cross entropy loss\n        c_loss = loss.data.item()\n        ax1.plot(count, c_loss, 'r.')\n        loss_val += c_loss\n\n        optimizer.zero_grad() #Zero out any cached gradients\n        loss.backward() #Backward pass\n        optimizer.step() #Update the weights\n\n        total_batch = (target.size(0) * target.size(1))\n        total += total_batch\n        output_data = torch.sigmoid(output)>=0.5\n        target_data = (target==1.0)\n        for arr1,arr2 in zip(output_data, target_data):\n            all_outputs.append(list(arr1.cpu().numpy()))\n            all_targets.append(list(arr2.cpu().numpy()))\n        c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\n        ax2.plot(count, c_acc/total_batch, 'r.')\n        correct += c_acc\n        count +=1\n        \n    all_outputs = np.array(all_outputs)\n    all_targets = np.array(all_targets)\n    f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\n    f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\n    recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\n    \n    f1_scores[\"samples_train\"].append(f1score_samples)\n    f1_scores[\"macro_train\"].append(f1score_macro)\n    f1_scores[\"weighted_train\"].append(f1score_weighted)\n    f1_scores[\"hamming_train\"].append(hamming)\n    \n    train_loss_val, train_iter, train_acc = loss_val/len(train_loader.dataset), count, correct/float(total)\n    \n    print(\"Training loss: \", train_loss_val, \" train acc: \",train_acc)    \n    ## Test Phase\n    \n    #Model switches to test phase\n    model.eval()\n    \n    all_outputs = []\n    all_targets = []\n    #Running through all mini batches in the dataset\n    count, correct, total, lost_val = test_iter, 0, 0, 0\n    for img_data, target in tqdm_notebook(val_loader, desc='Testing'):\n        img_data, target = img_data.to(device), target.to(device)\n        output = model(img_data)\n        loss = criterion(output, target) #Cross entropy loss\n        c_loss = loss.data.item()\n        ax3.plot(count, c_loss, 'b.')\n        loss_val += c_loss\n        #Compute accuracy\n        #predicted = output.data.max(1)[1] #get index of max\n        total_batch = (target.size(0) * target.size(1))\n        total += total_batch\n        output_data = torch.sigmoid(output)>=0.5\n        target_data = (target==1.0)\n        #print(\"Predictions: \", output_data)\n        #print(\"Actual: \", target_data)\n        for arr1,arr2 in zip(output_data, target_data):\n            all_outputs.append(list(arr1.cpu().numpy()))\n            all_targets.append(list(arr2.cpu().numpy()))\n        c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\n        ax4.plot(count, c_acc/total_batch, 'b.')\n        correct += c_acc\n        count += 1\n    \n    #print(\"Outputs: \", len(all_outputs), \" x \", len(all_outputs[0]))\n    #print(\"Targets: \", len(all_targets), \" x \", len(all_targets[0]))\n    \n    #F1 Score\n    all_outputs = np.array(all_outputs)\n    all_targets = np.array(all_targets)\n    f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\n    f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\n    recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n    hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\n    \n    f1_scores[\"samples_test\"].append(f1score_samples)\n    f1_scores[\"macro_test\"].append(f1score_macro)\n    f1_scores[\"weighted_test\"].append(f1score_weighted)\n    f1_scores[\"hamming_test\"].append(hamming)\n    \n    #Accuracy over entire dataset\n    test_acc, test_iter, test_loss_val = correct/float(total), count, loss_val/len(test_loader.dataset)\n    print(\"Test set accuracy: \",test_acc)\n    \n    train_results['epoch'].append(i)\n    train_results['train_loss'].append(train_loss_val)\n    train_results['train_acc'].append(train_acc)\n    train_results['train_iter'].append(train_iter)\n    \n    train_results['test_loss'].append(test_loss_val)\n    train_results['test_acc'].append(test_acc)\n    train_results['test_iter'].append(test_iter)\n    \n    #Save model with best accuracy\n    if test_acc > best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), 'best_model.pth') \nfig.savefig('train_curves.png')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:43:17.669002Z","iopub.execute_input":"2022-03-11T06:43:17.669516Z","iopub.status.idle":"2022-03-11T07:26:48.025768Z","shell.execute_reply.started":"2022-03-11T06:43:17.669477Z","shell.execute_reply":"2022-03-11T07:26:48.025103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TRAIN\")\nprint(\"F1 Samples: \", f1_scores[\"samples_train\"])\nprint(\"F1 Weighted: \", f1_scores[\"weighted_train\"])\nprint(\"Hamming: \", f1_scores[\"hamming_train\"])\nprint()\nprint(\"==============\")\nprint(\"VALIDATION\")\nprint(\"F1 Samples: \", f1_scores[\"samples_test\"])\nprint(\"F1 Weighted: \", f1_scores[\"weighted_test\"])\nprint(\"Hamming: \", f1_scores[\"hamming_test\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:26:48.027058Z","iopub.execute_input":"2022-03-11T07:26:48.027676Z","iopub.status.idle":"2022-03-11T07:26:48.037427Z","shell.execute_reply.started":"2022-03-11T07:26:48.027633Z","shell.execute_reply":"2022-03-11T07:26:48.036757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inference on test\nmodel_path = \"best_model.pth\"\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()\n\n#Run predictions\nall_outputs = []\nall_targets = []\nfor img_data, target in tqdm_notebook(test_loader, desc='Testing'):\n    img_data, target = img_data.to(device), target.to(device)\n    output = model(img_data)\n    loss = criterion(output, target) #Cross entropy loss\n    c_loss = loss.data.item()\n    ax3.plot(count, c_loss, 'b.')\n    loss_val += c_loss\n    #Compute accuracy\n    #predicted = output.data.max(1)[1] #get index of max\n    total_batch = (target.size(0) * target.size(1))\n    total += total_batch\n    output_data = torch.sigmoid(output)>=0.5\n    target_data = (target==1.0)\n    #print(\"Predictions: \", output_data)\n    #print(\"Actual: \", target_data)\n    for arr1,arr2 in zip(output_data, target_data):\n        all_outputs.append(list(arr1.cpu().numpy()))\n        all_targets.append(list(arr2.cpu().numpy()))\n    c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\n    ax4.plot(count, c_acc/total_batch, 'b.')\n    correct += c_acc\n    count += 1\n\n\n#F1 Score\nall_outputs = np.array(all_outputs)\nall_targets = np.array(all_targets)\nf1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\nf1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\nf1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\nrecall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\nprec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\nhamming = hamming_score(y_true=all_targets, y_pred=all_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:26:48.038879Z","iopub.execute_input":"2022-03-11T07:26:48.039683Z","iopub.status.idle":"2022-03-11T07:34:39.183215Z","shell.execute_reply.started":"2022-03-11T07:26:48.039644Z","shell.execute_reply":"2022-03-11T07:34:39.182506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TEST\")\nprint(\"F1 Samples: \", f1score_samples)\nprint(\"F1 Weighted: \", f1score_weighted)\nprint(\"Hamming: \", hamming)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:34:39.184677Z","iopub.execute_input":"2022-03-11T07:34:39.185003Z","iopub.status.idle":"2022-03-11T07:34:39.190999Z","shell.execute_reply.started":"2022-03-11T07:34:39.184963Z","shell.execute_reply":"2022-03-11T07:34:39.190217Z"},"trusted":true},"execution_count":null,"outputs":[]}]}